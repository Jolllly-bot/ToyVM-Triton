{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jolllly-bot/ToyVM-Triton/blob/main/Toy_Triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVTdwJHQRspr"
      },
      "source": [
        "# ToyVM Triton\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTnIqde_PmYv",
        "outputId": "4e054c9e-9e8d-4e27-c572-f739fb858a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu122\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu122-0.19.dev58-cp310-cp310-manylinux_2_28_x86_64.whl (1454.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 GB\u001b[0m \u001b[31m698.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (24.3.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu122) (4.12.2)\n",
            "Installing collected packages: mlc-ai-nightly-cu122\n",
            "Successfully installed mlc-ai-nightly-cu122-0.19.dev58\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --pre -U -f https://mlc.ai/wheels mlc-ai-nightly-cu122"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juRwkexOS1x1"
      },
      "source": [
        "## JIT Decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AM0Y9TUlPtWH"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import ast\n",
        "\n",
        "def jit(target=\"cpu\", verbose=True):\n",
        "    assert target in [\"cpu\", \"gpu\"]\n",
        "    def inner(fn):\n",
        "        return JIT(fn, target=target, verbose=verbose)\n",
        "    return inner\n",
        "\n",
        "class JIT:\n",
        "    def __init__(self, fn, target=\"cpu\", verbose=True):\n",
        "        self.fn = fn\n",
        "        self.target = target\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        fn_src = inspect.getsource(self.fn)\n",
        "        fn_ast = ast.parse(fn_src)\n",
        "        if self.verbose:\n",
        "          print(ast.dump(fn_ast))\n",
        "\n",
        "        ctx = self.fn.__globals__.copy()\n",
        "        code_generator = CodeGenerator(fn_ast, ctx, self.target, self.verbose)\n",
        "        compiled_kernel = code_generator.code_gen()\n",
        "\n",
        "        input_args = []\n",
        "        for arg in args:\n",
        "            input_args.append(arg.data)\n",
        "        return compiled_kernel(*input_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kikdnrAS8cU"
      },
      "source": [
        "## Frontend Lexer & Parser\n",
        "https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py\n",
        "https://github.com/triton-lang/triton/blob/main/python/triton/compiler/code_generator.py\n",
        "\n",
        "To simplify, sema is not implemented https://github.com/triton-lang/triton/blob/main/python/triton/language/semantic.py\n",
        "\n",
        "GPU runtime:\n",
        "https://mlc.ai/chapter_gpu_acceleration/part1.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ohiPSm_bRJgJ"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any, Type\n",
        "import astunparse\n",
        "import tvm\n",
        "from tvm import dlight as dl\n",
        "from tvm import relax\n",
        "from tvm.script import relax as R\n",
        "from tvm.script.ir_builder import relax as relax_builder, ir as I, IRBuilder\n",
        "\n",
        "class CodeGenerator(ast.NodeVisitor):\n",
        "    def __init__(self, fn_ast, ctx, target, verbose):\n",
        "        self.fn_ast = fn_ast\n",
        "        self.target = target\n",
        "        self.ib = IRBuilder()\n",
        "        self.ir_module = None\n",
        "        self.entry = None\n",
        "        self.ret = None\n",
        "        self.local_var_table : Dict[str, Any] = {}\n",
        "        self.ctx = ctx\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def code_gen(self):\n",
        "        with self.ib:\n",
        "            self.visit(self.fn_ast)\n",
        "        module = self.ib.get()\n",
        "        if self.verbose:\n",
        "          print(\"=========TVM IR=========\")\n",
        "          print(module)\n",
        "\n",
        "        # apply transform pass on module\n",
        "        with tvm.transform.PassContext(opt_level=3):\n",
        "            seq = tvm.transform.Sequential(\n",
        "                [\n",
        "                    # relax.transform.ConvertToDataflow(),\n",
        "                    relax.transform.LegalizeOps(),\n",
        "                    # relax.transform.AnnotateTIROpPattern(),\n",
        "                    relax.transform.FoldConstant(), # Fold constant expressions\n",
        "\n",
        "                    relax.transform.FuseOps(),\n",
        "                    relax.transform.FuseTIR(),\n",
        "                ])\n",
        "            module = seq(module)\n",
        "        if self.verbose:\n",
        "          print(\"===>After applied passes...\")\n",
        "          print(module)\n",
        "\n",
        "        mapped_target = {'cpu': 'llvm', 'gpu': 'cuda'}\n",
        "        target = tvm.target.Target(mapped_target[self.target])\n",
        "        if \"cuda\" in target.keys:\n",
        "          with target:\n",
        "            module = dl.ApplyDefaultSchedule(dl.gpu.Fallback(),)(module)\n",
        "          if self.verbose:\n",
        "            print(\"===>After ApplyDefaultSchedule...\")\n",
        "            print(module)\n",
        "          device = tvm.cuda()\n",
        "        else:\n",
        "          device = tvm.cpu()\n",
        "\n",
        "        with tvm.transform.PassContext(opt_level=3):\n",
        "            ex = relax.build(module, target=target)\n",
        "\n",
        "        if self.verbose and \"cuda\" in target.keys:\n",
        "          print(\"=========CUDA CODE=========\")\n",
        "          print(ex.mod.imported_modules[0].imported_modules[0].get_source())\n",
        "\n",
        "        vm = relax.VirtualMachine(ex, device=device)\n",
        "        return vm[self.entry]\n",
        "\n",
        "    def visit(self, node):\n",
        "        print(\"Visit \" + node.__class__.__name__)\n",
        "        return super().visit(node)\n",
        "\n",
        "    def visit_Module(self, node: ast.Module):\n",
        "        if self.ir_module:\n",
        "            raise AssertionError(\"We should have only one module!\")\n",
        "        self.ir_module = I.ir_module()\n",
        "        with self.ir_module:\n",
        "            super().generic_visit(node)\n",
        "\n",
        "\n",
        "    def visit_FunctionDef(self, node: ast.FunctionDef):\n",
        "        fn = relax_builder.function()\n",
        "        self.entry = node.name\n",
        "        print(\"entry Function: {}\".format(node.name))\n",
        "        with fn:\n",
        "            R.func_name(node.name)\n",
        "            self.visit(node.args)\n",
        "            self._visit_compound_stmt(node.body)\n",
        "\n",
        "            if self.ret is None:\n",
        "                R.func_ret_value(relax.ShapeExpr([]))\n",
        "            else:\n",
        "                R.func_ret_value(self.ret)\n",
        "\n",
        "    def visit_arguments(self, node: ast.arguments):\n",
        "      for arg in node.args:\n",
        "          arg_name = arg.arg\n",
        "          if arg.annotation is None:\n",
        "                raise ValueError(arg, \"Type annotation is required for function parameters.\")\n",
        "          anno = eval(astunparse.unparse(arg.annotation), self.ctx)\n",
        "          print(anno)\n",
        "          param = R.arg(arg_name, R.Tensor(shape=anno.shape, dtype=anno.dtype))\n",
        "          self.local_var_table[arg_name] = param\n",
        "\n",
        "    def _visit_compound_stmt(self, stmts):\n",
        "        assert isinstance(stmts, (list, tuple))\n",
        "        for stmt in stmts:\n",
        "            ret = self.visit(stmt)\n",
        "            if ret is not None and isinstance(stmt, ast.Return):\n",
        "                self.ret = ret\n",
        "\n",
        "    def visit_Pass(self, node: ast.Pass):\n",
        "        pass\n",
        "\n",
        "    def visit_Assign(self, node: ast.Assign):\n",
        "        if len(node.targets) != 1:\n",
        "            raise NotImplementedError(\"Doesn't support simultaneous multiple assignment like 'a = b = c' in AST node type: {}\".format(type(node).__name__))\n",
        "        target: relax.Var = self.visit(node.targets[0])\n",
        "        value = self.visit(node.value)\n",
        "        self.local_var_table[target.name_hint] = value\n",
        "        self.ib.name(target.name_hint, value)\n",
        "\n",
        "    def visit_Name(self, node: ast.Name):\n",
        "        name = node.id\n",
        "        if isinstance(node.ctx, ast.Store):\n",
        "            if name not in self.local_var_table.keys():\n",
        "                self.local_var_table[name] = relax.Var(name, struct_info=relax.ObjectStructInfo())\n",
        "        return self.local_var_table[name]\n",
        "\n",
        "    def visit_BinOp(self, node: ast.BinOp):\n",
        "        lhs = self.visit(node.left)\n",
        "        rhs = self.visit(node.right)\n",
        "        # TODO: refactor\n",
        "        method_name = self._method_name_for_bin_op.get(type(node.op))\n",
        "        print(\"method name\", method_name)\n",
        "\n",
        "        if isinstance(node.op, ast.Add):\n",
        "            return R.emit(R.add(lhs, rhs))\n",
        "        elif isinstance(node.op, ast.Mult):\n",
        "            return R.emit(R.multiply(lhs, rhs))\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))\n",
        "\n",
        "    _method_name_for_bin_op: Dict[Type[ast.operator], str] = {\n",
        "            ast.Add: '__add__',\n",
        "            ast.Sub: '__sub__',\n",
        "            ast.Mult: '__mul__',\n",
        "            ast.Div: '__truediv__',\n",
        "            ast.FloorDiv: '__floordiv__',\n",
        "            ast.Mod: '__mod__',\n",
        "            ast.Pow: '__pow__',\n",
        "            ast.LShift: '__lshift__',\n",
        "            ast.RShift: '__rshift__',\n",
        "            ast.BitAnd: '__and__',\n",
        "            ast.BitOr: '__or__',\n",
        "            ast.BitXor: '__xor__',\n",
        "        }\n",
        "\n",
        "\n",
        "    def visit_Return(self, node: ast.Return):\n",
        "        ret_value = self.visit(node.value)\n",
        "        return ret_value\n",
        "\n",
        "    def visit_Constant(self, node: ast.Constant):\n",
        "        return R.emit(relax.const(node.value))\n",
        "\n",
        "    def generic_visit(self, node: ast.AST):\n",
        "        raise NotImplementedError(\"Unsupported AST node type: {}\".format(type(node).__name__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRm8OU5VK4Z"
      },
      "source": [
        "## Tensor Definition using [DLPack](https://dmlc.github.io/dlpack/latest/)\n",
        "\n",
        "https://dmlc.github.io/dlpack/latest/python_spec.html\n",
        "Tensor usage Test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rWgbnHy_U6GI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class Tensor:\n",
        "    def __init__(self, shape, dtype):\n",
        "        self.shape = shape\n",
        "        self.dtype = dtype\n",
        "        self._data = None\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self._data\n",
        "\n",
        "    @data.setter\n",
        "    def data(self, data: \"torch.Tensor\"):\n",
        "        def _from_dlpack(tensor):\n",
        "            from tvm.runtime import Device\n",
        "            from tvm.runtime import ndarray\n",
        "            try:\n",
        "                return ndarray.from_dlpack(tensor)\n",
        "            except RuntimeError:\n",
        "                pass\n",
        "            device_type = tensor.device.type\n",
        "            device_id = tensor.device.index or 0\n",
        "            return ndarray.array(\n",
        "                tensor.numpy(),\n",
        "                device=Device(\n",
        "                    Device.STR2MASK[device_type],\n",
        "                    device_id,\n",
        "                ),\n",
        "            )\n",
        "        data = _from_dlpack(data)\n",
        "        if data.shape != tuple(self.shape):\n",
        "            raise ValueError(f\"Shape mismatch: expected {tuple(self.shape)}, got {data.shape}\")\n",
        "        if data.dtype != self.dtype:\n",
        "            raise ValueError(f\"Dtype mismatch: expected {self.dtype}, got {data.dtype}\")\n",
        "        self._data = data\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.dtype) + '[' + ', '.join(str(s) for s in self.shape) + ']'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dxgo2lu7U-iL",
        "outputId": "247114c9-50f9-420f-e71e-7481a6d17a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32[2, 3]\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "<class 'tvm.runtime.ndarray.NDArray'>\n"
          ]
        }
      ],
      "source": [
        "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "a.data = torch.ones(size=(2, 3), dtype=torch.float32)\n",
        "print(a)\n",
        "print(a.data)\n",
        "print(type(a.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzwLoMrAVaTp"
      },
      "source": [
        "## Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aXg_4Z68Ww4H",
        "outputId": "9c3c24fb-ac43-4b17-8b9a-c2493d16a082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module(body=[FunctionDef(name='add_tensor', args=arguments(posonlyargs=[], args=[arg(arg='a', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))])), arg(arg='b', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))]))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='out', ctx=Store())], value=BinOp(left=Name(id='a', ctx=Load()), op=Add(), right=Name(id='b', ctx=Load()))), Return(value=Name(id='out', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='cpu'))])])], type_ignores=[])\n",
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: add_tensor\n",
            "Visit arguments\n",
            "float32[2, 3]\n",
            "float32[2, 3]\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Name\n",
            "Visit Name\n",
            "method name __add__\n",
            "Visit Return\n",
            "Visit Name\n",
            "=========TVM IR=========\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def add_tensor(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
            "        out: R.Tensor((2, 3), dtype=\"float32\") = R.add(a, b)\n",
            "        return out\n",
            "===>After applied passes...\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func(private=True)\n",
            "    def add(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_add: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
            "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
            "        # with T.block(\"root\"):\n",
            "        for ax0, ax1 in T.grid(T.int64(2), T.int64(3)):\n",
            "            with T.block(\"T_add\"):\n",
            "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
            "                T.reads(a[v_ax0, v_ax1], b[v_ax0, v_ax1])\n",
            "                T.writes(T_add[v_ax0, v_ax1])\n",
            "                T_add[v_ax0, v_ax1] = a[v_ax0, v_ax1] + b[v_ax0, v_ax1]\n",
            "\n",
            "    @R.function\n",
            "    def add_tensor(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
            "        cls = Module\n",
            "        out = R.call_tir(cls.add, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
            "        return out\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]]\n"
          ]
        }
      ],
      "source": [
        "@jit(target=\"cpu\")\n",
        "def add_tensor(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")):\n",
        "    out = a + b\n",
        "    return out\n",
        "\n",
        "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "a.data = torch.ones(size=(2, 3), dtype=torch.float32)\n",
        "b.data = torch.ones(size=(2, 3), dtype=torch.float32)\n",
        "print(add_tensor(a, b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pUYWd3WkOTi7",
        "outputId": "2ecc0003-e343-4477-8a55-8713167a75e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: add_tensor\n",
            "Visit arguments\n",
            "float32[2, 3]\n",
            "float32[2, 3]\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Name\n",
            "Visit Name\n",
            "method name __add__\n",
            "Visit Return\n",
            "Visit Name\n",
            "[[2. 2. 2.]\n",
            " [2. 2. 2.]]\n"
          ]
        }
      ],
      "source": [
        "@jit(target=\"gpu\", verbose=False)\n",
        "def add_tensor(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")):\n",
        "    out = a + b\n",
        "    return out\n",
        "\n",
        "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "a.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
        "b.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
        "print(add_tensor(a, b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qpSQP__nQqD4",
        "outputId": "adde8da0-08b5-4ba3-bacd-61c8c44fee77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module(body=[FunctionDef(name='mul_tensor', args=arguments(posonlyargs=[], args=[arg(arg='a', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))])), arg(arg='b', annotation=Call(func=Name(id='Tensor', ctx=Load()), args=[], keywords=[keyword(arg='shape', value=Tuple(elts=[Constant(value=2), Constant(value=3)], ctx=Load())), keyword(arg='dtype', value=Constant(value='float32'))]))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='out', ctx=Store())], value=BinOp(left=Name(id='a', ctx=Load()), op=Mult(), right=Name(id='b', ctx=Load()))), Return(value=Name(id='out', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='gpu'))])])], type_ignores=[])\n",
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: mul_tensor\n",
            "Visit arguments\n",
            "float32[2, 3]\n",
            "float32[2, 3]\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Name\n",
            "Visit Name\n",
            "method name __mul__\n",
            "Visit Return\n",
            "Visit Name\n",
            "=========TVM IR=========\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def mul_tensor(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
            "        out: R.Tensor((2, 3), dtype=\"float32\") = R.multiply(a, b)\n",
            "        return out\n",
            "===>After applied passes...\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func(private=True)\n",
            "    def multiply(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_multiply: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
            "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
            "        # with T.block(\"root\"):\n",
            "        for ax0, ax1 in T.grid(T.int64(2), T.int64(3)):\n",
            "            with T.block(\"T_multiply\"):\n",
            "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
            "                T.reads(a[v_ax0, v_ax1], b[v_ax0, v_ax1])\n",
            "                T.writes(T_multiply[v_ax0, v_ax1])\n",
            "                T_multiply[v_ax0, v_ax1] = a[v_ax0, v_ax1] * b[v_ax0, v_ax1]\n",
            "\n",
            "    @R.function\n",
            "    def mul_tensor(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
            "        cls = Module\n",
            "        out = R.call_tir(cls.multiply, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
            "        return out\n",
            "===>After ApplyDefaultSchedule...\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func(private=True)\n",
            "    def multiply(a: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), b: T.Buffer((T.int64(2), T.int64(3)), \"float32\"), T_multiply: T.Buffer((T.int64(2), T.int64(3)), \"float32\")):\n",
            "        T.func_attr({\"tir.is_scheduled\": 1, \"tir.noalias\": T.bool(True)})\n",
            "        # with T.block(\"root\"):\n",
            "        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread=\"blockIdx.x\"):\n",
            "            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n",
            "                with T.block(\"T_multiply\"):\n",
            "                    v0 = T.axis.spatial(T.int64(2), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(3))\n",
            "                    v1 = T.axis.spatial(T.int64(3), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(3))\n",
            "                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(6))\n",
            "                    T.reads(a[v0, v1], b[v0, v1])\n",
            "                    T.writes(T_multiply[v0, v1])\n",
            "                    T_multiply[v0, v1] = a[v0, v1] * b[v0, v1]\n",
            "\n",
            "    @R.function\n",
            "    def mul_tensor(a: R.Tensor((2, 3), dtype=\"float32\"), b: R.Tensor((2, 3), dtype=\"float32\")) -> R.Tensor((2, 3), dtype=\"float32\"):\n",
            "        cls = Module\n",
            "        out = R.call_tir(cls.multiply, (a, b), out_sinfo=R.Tensor((2, 3), dtype=\"float32\"))\n",
            "        return out\n",
            "=========CUDA CODE=========\n",
            "\n",
            "#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n",
            "     (__CUDACC_VER_MAJOR__ > 11))\n",
            "#define TVM_ENABLE_L2_PREFETCH 1\n",
            "#else\n",
            "#define TVM_ENABLE_L2_PREFETCH 0\n",
            "#endif\n",
            "\n",
            "#ifdef _WIN32\n",
            "  using uint = unsigned int;\n",
            "  using uchar = unsigned char;\n",
            "  using ushort = unsigned short;\n",
            "  using int64_t = long long;\n",
            "  using uint64_t = unsigned long long;\n",
            "#else\n",
            "  #define uint unsigned int\n",
            "  #define uchar unsigned char\n",
            "  #define ushort unsigned short\n",
            "  #define int64_t long long\n",
            "  #define uint64_t unsigned long long\n",
            "#endif\n",
            "extern \"C\" __global__ void __launch_bounds__(1024) multiply_kernel(float* __restrict__ T_multiply, float* __restrict__ a, float* __restrict__ b);\n",
            "extern \"C\" __global__ void __launch_bounds__(1024) multiply_kernel(float* __restrict__ T_multiply, float* __restrict__ a, float* __restrict__ b) {\n",
            "  if (((int)threadIdx.x) < 6) {\n",
            "    T_multiply[((int)threadIdx.x)] = (a[((int)threadIdx.x)] * b[((int)threadIdx.x)]);\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n"
          ]
        }
      ],
      "source": [
        "@jit(target=\"gpu\")\n",
        "def mul_tensor(a: Tensor(shape=(2, 3), dtype=\"float32\"), b: Tensor(shape=(2, 3), dtype=\"float32\")):\n",
        "    out = a * b\n",
        "    return out\n",
        "\n",
        "a = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "b = Tensor(shape=(2, 3), dtype=\"float32\")\n",
        "a.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
        "b.data = torch.ones(size=(2, 3), dtype=torch.float32, device=\"cuda\")\n",
        "print(mul_tensor(a, b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oKbg4u9_VCvs",
        "outputId": "03e46048-2d9a-4252-8ca2-d4a0fa94b65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: mul_tensor\n",
            "Visit arguments\n",
            "float32[2, 3]\n",
            "float32[2, 3]\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Name\n",
            "Visit Name\n",
            "method name __mul__\n",
            "Visit Return\n",
            "Visit Name\n",
            "Your add time: 0.010741 seconds\n",
            "Torch add time: 0.015473 seconds\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "import inspect\n",
        "\n",
        "\n",
        "fn_src = inspect.getsource(mul_tensor.fn)  # Get source code of the original function\n",
        "fn_ast = ast.parse(fn_src)\n",
        "ctx = mul_tensor.fn.__globals__.copy()\n",
        "code_generator = CodeGenerator(fn_ast, ctx, target=\"gpu\", verbose=False)  # Create CodeGenerator instance\n",
        "compiled_kernel = code_generator.code_gen()  # Get compiled kernel\n",
        "\n",
        "# Time your add function (execution only)\n",
        "time_your_add = timeit.timeit(lambda: compiled_kernel(a.data, b.data), number=1000)\n",
        "\n",
        "# Convert NDArrays back to PyTorch tensors for torch.add\n",
        "a_tensor = torch.from_numpy(a.data.numpy())\n",
        "b_tensor = torch.from_numpy(b.data.numpy())\n",
        "\n",
        "# Time Torch's add function\n",
        "time_torch_add = timeit.timeit(lambda: torch.mul(a_tensor, b_tensor), number=1000)\n",
        "\n",
        "print(f\"Your mul time: {time_your_add:.6f} seconds\")\n",
        "print(f\"Torch mul time: {time_torch_add:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaGdK_T0GQod"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "import inspect\n",
        "\n",
        "\n",
        "fn_src = inspect.getsource(add_tensor.fn)  # Get source code of the original function\n",
        "fn_ast = ast.parse(fn_src)\n",
        "ctx = mul_tensor.fn.__globals__.copy()\n",
        "code_generator = CodeGenerator(fn_ast, ctx, target=\"gpu\", verbose=False)  # Create CodeGenerator instance\n",
        "compiled_kernel = code_generator.code_gen()  # Get compiled kernel\n",
        "\n",
        "# Time your add function (execution only)\n",
        "time_your_add = timeit.timeit(lambda: compiled_kernel(a.data, b.data), number=1000)\n",
        "\n",
        "# Convert NDArrays back to PyTorch tensors for torch.add\n",
        "a_tensor = torch.from_numpy(a.data.numpy())\n",
        "b_tensor = torch.from_numpy(b.data.numpy())\n",
        "\n",
        "# Time Torch's add function\n",
        "time_torch_add = timeit.timeit(lambda: torch.add(a_tensor, b_tensor), number=1000)\n",
        "\n",
        "print(f\"Your add time: {time_your_add:.6f} seconds\")\n",
        "print(f\"Torch add time: {time_torch_add:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CKoMwJjDRlSf",
        "outputId": "e3939f64-8ecf-4808-e5ca-49f5e263b2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module(body=[FunctionDef(name='add', args=arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='add', ctx=Store())], value=BinOp(left=Constant(value=1), op=Add(), right=Constant(value=1))), Assign(targets=[Name(id='res', ctx=Store())], value=BinOp(left=Name(id='add', ctx=Load()), op=Add(), right=Constant(value=1))), Return(value=Name(id='res', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='cpu'))])])], type_ignores=[])\n",
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: add\n",
            "Visit arguments\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Constant\n",
            "Visit Constant\n",
            "method name __add__\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Name\n",
            "Visit Constant\n",
            "method name __add__\n",
            "Visit Return\n",
            "Visit Name\n",
            "=========TVM IR=========\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def add() -> R.Tensor((), dtype=\"int32\"):\n",
            "        gv: R.Tensor((), dtype=\"int32\") = R.const(1, \"int32\")\n",
            "        gv1: R.Tensor((), dtype=\"int32\") = R.const(1, \"int32\")\n",
            "        add: R.Tensor((), dtype=\"int32\") = R.add(gv, gv1)\n",
            "        gv3: R.Tensor((), dtype=\"int32\") = R.const(1, \"int32\")\n",
            "        res: R.Tensor((), dtype=\"int32\") = R.add(add, gv3)\n",
            "        return res\n",
            "===>After applied passes...\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def add() -> R.Tensor((), dtype=\"int32\"):\n",
            "        return R.const(3, \"int32\")\n",
            "Add Result: 3\n"
          ]
        }
      ],
      "source": [
        "@jit(target=\"cpu\")\n",
        "def add():\n",
        "    add = 1 + 1\n",
        "    res = add + 1\n",
        "    return res\n",
        "print(\"Add Result:\", add())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Amg5wILRWy3l",
        "outputId": "60e2854f-cf57-492c-ee15-53c145a048df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Module(body=[FunctionDef(name='mul', args=arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='res', ctx=Store())], value=BinOp(left=Constant(value=1), op=Mult(), right=Constant(value=1))), Return(value=Name(id='res', ctx=Load()))], decorator_list=[Call(func=Name(id='jit', ctx=Load()), args=[], keywords=[keyword(arg='target', value=Constant(value='cpu'))])])], type_ignores=[])\n",
            "Visit Module\n",
            "Visit FunctionDef\n",
            "entry Function: mul\n",
            "Visit arguments\n",
            "Visit Assign\n",
            "Visit Name\n",
            "Visit BinOp\n",
            "Visit Constant\n",
            "Visit Constant\n",
            "method name __mul__\n",
            "Visit Return\n",
            "Visit Name\n",
            "=========TVM IR=========\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def mul() -> R.Tensor((), dtype=\"int32\"):\n",
            "        gv: R.Tensor((), dtype=\"int32\") = R.const(1, \"int32\")\n",
            "        gv1: R.Tensor((), dtype=\"int32\") = R.const(1, \"int32\")\n",
            "        res: R.Tensor((), dtype=\"int32\") = R.multiply(gv, gv1)\n",
            "        return res\n",
            "===>After applied passes...\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import relax as R\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @R.function\n",
            "    def mul() -> R.Tensor((), dtype=\"int32\"):\n",
            "        return R.const(1, \"int32\")\n",
            "Mul Result: 1\n"
          ]
        }
      ],
      "source": [
        "@jit(target=\"cpu\")\n",
        "def mul():\n",
        "    res = 1 * 1\n",
        "    return res\n",
        "\n",
        "print(\"Mul Result:\", mul())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J0Jy-8RRxqy"
      },
      "source": [
        "dot product, 1d conv, matmul"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzmZAGx0oo++cXrzxAyPQ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}